{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOZ1KPojKeuKWlruiZcCClE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cmari038/Language-Translator/blob/main/RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision torchaudio\n",
        "!pip install spacy\n",
        "#!pip install collections"
      ],
      "metadata": {
        "id": "MmcSj7k0iOb2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy download es_core_news_sm"
      ],
      "metadata": {
        "id": "xqCfxMAWw9R1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchtext; torchtext.disable_torchtext_deprecation_warning()\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import spacy\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as functional\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torch.utils.data import random_split\n",
        "from torchtext.vocab import vocab\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from collections import Counter, OrderedDict"
      ],
      "metadata": {
        "id": "oiDcQWEB7eTM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# connect pytorch to cuda\n",
        "device = torch.device(\"cuda\")\n",
        "\n",
        "dataset = 'https://raw.githubusercontent.com/cmari038/Language-Translator/main/data.csv'\n",
        "data = pd.read_csv(dataset)\n",
        "\n",
        "#spacy.load('en_core_web_sm')\n",
        "#spacy.load('es_core_news_sm')\n",
        "\n",
        "# processing data\n",
        "\n",
        "english_tokenizer = get_tokenizer('spacy', language = 'en_core_web_sm')\n",
        "spanish_tokenizer = get_tokenizer('spacy', language= 'es_core_news_sm')\n",
        "\n",
        "#print(data)\n",
        "\n",
        "#train = data.sample(frac=0.7)\n",
        "#validate = data.drop(train.index).sample(frac=0.1)\n",
        "#test = data.drop(validate.index)\n",
        "\n",
        "train = data.iloc[:int(len(data) * 0.80)]\n",
        "\n",
        "en_lowercase = []\n",
        "for element in train['english']:\n",
        "  en_lowercase.append(element.lower())\n",
        "\n",
        "train.loc[:, 'english'] = en_lowercase\n",
        "\n",
        "es_lowercase = []\n",
        "for element in train['spanish']:\n",
        "  es_lowercase.append(element.lower())\n",
        "\n",
        "train.loc[:,'spanish'] = es_lowercase\n",
        "\n",
        "#train.reset_index(drop=True, inplace=True)\n",
        "#validate.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# Build vocab structure to store tokens and their corresponding index\n",
        "counter1 = Counter()\n",
        "counter2 = Counter()\n",
        "\n",
        "for sentence in train['english']:\n",
        "  counter1.update(english_tokenizer(sentence))\n",
        "\n",
        "for sentence in train['spanish']:\n",
        "  counter2.update(spanish_tokenizer(sentence))\n",
        "\n",
        "en_dict = OrderedDict(counter1.most_common())\n",
        "es_dict = OrderedDict(counter2.most_common())\n",
        "\n",
        "vocab1 = vocab(en_dict, specials = ['<unk>', '<pad>', '<sos>', '<eos>'])\n",
        "vocab2 = vocab(es_dict, specials = ['<unk>', '<pad>', '<sos>', '<eos>'])\n",
        "\n",
        "vocab1.set_default_index(vocab1['<unk>'])\n",
        "vocab2.set_default_index(vocab2['<unk>'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tV0KWu9MilTU",
        "outputId": "4491ed75-000d-42c3-cfee-c0879650ae3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/spacy/util.py:1740: UserWarning: [W111] Jupyter notebook detected: if using `prefer_gpu()` or `require_gpu()`, include it in the same cell right before `spacy.load()` to ensure that the model is loaded on the correct device. More information: http://spacy.io/usage/v3#jupyter-notebook-gpu\n",
            "  warnings.warn(Warnings.W111)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GrMEtXyVOpEf",
        "outputId": "ddbb1af5-c650-4408-a04b-593511eae075"
      },
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "118964\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# finding set of tokens for each phrase\n",
        "def getTokens(df, en_tokenizer, es_tokenizer, lang):\n",
        "  if lang == \"english\":\n",
        "    en_tokens = []\n",
        "\n",
        "    for token in en_tokenizer(df):\n",
        "      en_tokens.append(token)\n",
        "\n",
        "    en_tokens = ['<sos>'] + en_tokens + ['<eos>']\n",
        "\n",
        "    return en_tokens\n",
        "\n",
        "  else:\n",
        "    es_tokens = []\n",
        "\n",
        "    for token in es_tokenizer(df):\n",
        "        es_tokens.append(token)\n",
        "\n",
        "    es_tokens = ['sos'] + es_tokens + ['<eos>']\n",
        "\n",
        "    return es_tokens\n",
        "\n",
        "\n",
        "#token_dict = {\"en_tokenizer\": english_tokenizer, \"es_tokenizer\": spanish_tokenizer}\n",
        "#train = train.apply(map(lambda col: col.map(getTokens)))\n",
        "en_tokens = []\n",
        "es_tokens = []\n",
        "\n",
        "for element in train.loc[:,'english']:\n",
        "  tmp1 = getTokens(element, english_tokenizer, spanish_tokenizer, \"english\")\n",
        "  en_tokens.append(tmp1)\n",
        "\n",
        "for element in train.loc[:,\"spanish\"]:\n",
        "  tmp2 = getTokens(element, english_tokenizer, spanish_tokenizer, \"spanish\")\n",
        "  es_tokens.append(tmp2)\n",
        "\n",
        "train['en_tokens'] = en_tokens\n",
        "train['es_tokens'] = es_tokens\n",
        "\n",
        "#print(train['en_tokens'])"
      ],
      "metadata": {
        "id": "32pkZX7Z7RWE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f6bc8a1-4d85-4c9a-87ac-ea34b9adc1f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-142-4fdc740c78c6>:37: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  train['en_tokens'] = en_tokens\n",
            "<ipython-input-142-4fdc740c78c6>:38: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  train['es_tokens'] = es_tokens\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train['en_tokens'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y23V_B__irUt",
        "outputId": "0841ef28-0ed6-49cb-98c5-af216cdf413e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0                                    [<sos>, go, ., <eos>]\n",
            "1                                    [<sos>, go, ., <eos>]\n",
            "2                                    [<sos>, go, ., <eos>]\n",
            "3                                    [<sos>, go, ., <eos>]\n",
            "4                                    [<sos>, hi, ., <eos>]\n",
            "                               ...                        \n",
            "95166    [<sos>, i, thought, you, 'd, wear, something, ...\n",
            "95167    [<sos>, i, threw, the, strange, package, on, t...\n",
            "95168    [<sos>, i, told, tom, mary, would, n't, accept...\n",
            "95169    [<sos>, i, told, tom, that, he, could, n't, sp...\n",
            "95170    [<sos>, i, told, her, what, he, was, doing, in...\n",
            "Name: en_tokens, Length: 95171, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def getIndices(df, en_vocab, es_vocab, lang):\n",
        "  if lang == \"english\":\n",
        "    en_indices = []\n",
        "\n",
        "    for word in df:\n",
        "      en_indices.append(en_vocab[word])\n",
        "\n",
        "    return en_indices\n",
        "\n",
        "  else:\n",
        "    es_indices = []\n",
        "\n",
        "    for word in df:\n",
        "        es_indices.append(es_vocab[word])\n",
        "\n",
        "    return es_indices\n",
        "\n",
        "en_indices = []\n",
        "es_indices = []\n",
        "\n",
        "for element in train.loc[:,\"en_tokens\"]:\n",
        "  tmp1 = getIndices(element, vocab1, vocab2, \"english\")\n",
        "  en_indices.append(tmp1)\n",
        "\n",
        "for element in train.loc[:, \"es_tokens\"]:\n",
        "  tmp1 = getIndices(element, vocab1, vocab2, \"spanish\")\n",
        "  es_indices.append(tmp1)\n",
        "\n",
        "train['es_indices'] = es_indices\n",
        "train['en_indices']= en_indices"
      ],
      "metadata": {
        "id": "-xUWIEhSSD7Z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273
        },
        "outputId": "af14b5f5-511d-4821-9285-afe4d3bde7e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-144-49ececf8f000>:29: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  train['es_indices'] = es_indices\n",
            "<ipython-input-144-49ececf8f000>:30: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  train['en_indices']= en_indices\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nprint(train['en_indices'])\\n\\nfor series in train['en_indices']:\\n  en_tensor.append(torch.tensor(series, dtype=torch.long))\\n\\nfor series in train['es_indices']:\\n  es_tensor.append(torch.tensor(series, dtype=torch.long))\\n\\ntrain['en_indices'] = en_tensor\\ntrain['es_indices'] = es_tensor\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 144
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train['en_indices'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6BHIg16OjM5w",
        "outputId": "19a9e6c4-f9c4-4d39-b4ec-bebd54a83070"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0                    [2, 15, 53, 37, 4434, 7, 85, 8, 4, 3]\n",
            "1                       [2, 27, 21, 1524, 7, 6, 994, 4, 3]\n",
            "2           [2, 5, 146, 8, 121, 9, 71, 101, 398, 32, 4, 3]\n",
            "3        [2, 5, 286, 39, 7, 150, 102, 108, 2890, 263, 4...\n",
            "4          [2, 255, 240, 6010, 22, 27, 17, 60, 1525, 4, 3]\n",
            "                               ...                        \n",
            "83270                    [2, 8, 23, 7, 220, 18, 375, 4, 3]\n",
            "83271           [2, 12, 25, 6, 311, 28, 442, 11741, 11, 3]\n",
            "83272    [2, 75, 10, 2588, 47, 14, 16, 22, 10, 578, 47,...\n",
            "83273    [2, 130, 2484, 185, 4930, 3231, 440, 22, 102, ...\n",
            "83274                               [2, 47, 29, 50, 11, 3]\n",
            "Name: en_indices, Length: 83275, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# convets dataframe to dataset for tensor acess\n",
        "class TensorSet(Dataset):\n",
        "  def __init__(self, df):\n",
        "    self.df = df\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.df)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    return torch.tensor(self.df['en_indices'].iloc[index], dtype=torch.long), torch.tensor(self.df['es_indices'].iloc[index], dtype=torch.long)\n",
        "\n",
        "def collate_fn(batch):\n",
        "  # used for making sure sequences are similar lengths by adding tokens to pad out the length\n",
        "  en_batch = []\n",
        "  es_batch = []\n",
        "  for en_sample, es_sample in batch:\n",
        "    en_batch.append(en_sample)\n",
        "    es_batch.append(es_sample)\n",
        "\n",
        "  en_batch = pad_sequence(en_batch, padding_value=vocab1['<pad>'])\n",
        "  es_batch = pad_sequence(es_batch, padding_value=vocab2['<pad>'])\n",
        "\n",
        "  return en_batch, es_batch\n",
        "\n",
        "#tensorSet = TensorSet(train, english_tokenizer, spanish_tokenizer, vocab1, vocab2)\n",
        "tensorSet = TensorSet(train)\n",
        "batch_size = 128\n",
        "dataLoad = DataLoader(tensorSet, shuffle=True, collate_fn=collate_fn, batch_size=batch_size)\n",
        "#evalDataLoad = DataLoader(TensorSet(validate), shuffle=True, collate_fn=collate_fn, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "Zh8-rGf-GpvV"
      },
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RNN model\n",
        "# made up of encoder, deocder, and seq2seq\n",
        "# encoder converts tensors into context vectors\n",
        "class RNN_Encoder(nn.Module):\n",
        "    def __init__(self, input, embedding_dimension, hidden_dimension, layers, dropout_p=0.5):\n",
        "        super(RNN_Encoder, self).__init__()\n",
        "        self.hidden_dimension = hidden_dimension\n",
        "        self.layers = layers\n",
        "        self.embed = nn.Embedding(input, embedding_dimension)\n",
        "        self.lstm = nn.LSTM(embedding_dimension, hidden_dimension, layers, dropout=dropout_p)\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "    def forward(self, english):\n",
        "        embedded = self.dropout(self.embed(english))\n",
        "        output, (hidden, cell) = self.lstm(embedded)\n",
        "        return hidden, cell\n",
        "\n",
        "# decoder uses context vectors to output spanish translation\n",
        "class RNN_Decoder(nn.Module):\n",
        "    def __init__(self, output, embedding_dimension, hidden_dimension, layers, dropout_p=0.5):\n",
        "      super(RNN_Decoder, self).__init__()\n",
        "      self.output = output\n",
        "      self.hidden_dimension = hidden_dimension\n",
        "      self.layers = layers\n",
        "      self.embed = nn.Embedding(output, hidden_dimension)\n",
        "      self.lstm = nn.LSTM(embedding_dimension, hidden_dimension, layers, dropout=dropout_p)\n",
        "      self.fc_out = nn.Linear(hidden_dimension, output)\n",
        "      self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "    def forward(self, input, hidden, cell):\n",
        "      input = input.unsqueeze(0)\n",
        "      embedded = self.dropout(self.embed(input))\n",
        "      out, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n",
        "      spanish_prediction = self.fc_out(out.squeeze(0))\n",
        "      #output = self.embed(input)\n",
        "      #output = functional.relu(output)\n",
        "      #output, hidden = self.gru(output, hidden)\n",
        "      return spanish_prediction, hidden, cell\n",
        "\n",
        "# sequence to sequence that ties encoder and decoder together\n",
        "class Sequence(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "      super(Sequence, self).__init__()\n",
        "      self.encoder = encoder\n",
        "      self.decoder = decoder\n",
        "      self.device = device\n",
        "\n",
        "    def forward(self, english, spanish, teacher_forcing_ratio):\n",
        "      batch_size = spanish.shape[1]\n",
        "      es_length = spanish.shape[0]\n",
        "      es_vocab_size = self.decoder.output\n",
        "      outputs = torch.zeros(es_length, batch_size,es_vocab_size).to(self.device)\n",
        "      hidden, cell = self.encoder(english)\n",
        "      input = spanish[0,:]\n",
        "\n",
        "      for i in range(1, es_length):\n",
        "        output, hidden, cell = self.decoder(input, hidden, cell)\n",
        "        outputs[i] = output\n",
        "        teacher_force = np.random.random() < teacher_forcing_ratio\n",
        "        top1 = output.argmax(1)\n",
        "        decoder_input = spanish[i] if teacher_force else top1\n",
        "\n",
        "      return outputs\n",
        "\n",
        "encoder = RNN_Encoder(input=len(vocab1), embedding_dimension=256, hidden_dimension=256, layers=2)\n",
        "decoder = RNN_Decoder(output=len(vocab2), embedding_dimension=256, hidden_dimension=256, layers=2)\n",
        "RNN_model = Sequence(encoder, decoder, device).to(device)"
      ],
      "metadata": {
        "id": "o-Bcb9Nspvh4"
      },
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss(ignore_index=vocab2['<pad>'])\n",
        "optimizer = torch.optim.Adam(RNN_model.parameters())\n",
        "\n",
        "# defines weights\n",
        "def weights(model):\n",
        "  for name, parameter in model.named_parameters():\n",
        "    nn.init.uniform_(parameter.data, -0.5, 0.5)\n",
        "\n",
        "def training(RNN_model, dataLoad, criterion, optimizer, device, epochs, teacher_forcing_ratio, clip):\n",
        "  #RNN_model.train()\n",
        "  for epoch in range(epochs):\n",
        "    RNN_model.train()\n",
        "    epoch_loss = 0\n",
        "    for (english, spanish) in dataLoad: # iterate in batches of size 128 through dataloader\n",
        "      english = english.to(device)\n",
        "      spanish = spanish.to(device)\n",
        "      optimizer.zero_grad()\n",
        "      # passing tensors into neural network\n",
        "      output = RNN_model(english, spanish, teacher_forcing_ratio)\n",
        "      output = output[1:].view(-1, output.shape[-1])\n",
        "      spanish = spanish[1:].view(-1)\n",
        "      # compares neural network's spanish translation to translation from dataset\n",
        "      loss = criterion(output, spanish)\n",
        "      loss.backward()\n",
        "      torch.nn.utils.clip_grad_norm_(RNN_model.parameters(),clip)\n",
        "      optimizer.step()\n",
        "      epoch_loss += loss.item()\n",
        "\n",
        "RNN_model.apply(weights)\n",
        "training(RNN_model, dataLoad, criterion, optimizer, device, 5, 0.5, 1.0)"
      ],
      "metadata": {
        "id": "zVJg6uIts0-D"
      },
      "execution_count": 158,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# translates an input sentece into spanish using trained model\n",
        "def translation(model, input, vocab1, vocab2, device):\n",
        "  input = input.lower()\n",
        "  #print(input)\n",
        "  model.eval()\n",
        "  #tokens = english_tokenizer(input)\n",
        "  #indices = []\n",
        "  \"\"\"for token in tokens:\n",
        "      indices.append(vocab1[token])\n",
        "  input_tensor = torch.tensor([vocab1['<sos>']] + indices + [vocab1['<eos>']], dtype=torch.long) \"\"\"\n",
        "\n",
        "  with torch.no_grad():\n",
        "    tokens = getTokens(input, english_tokenizer, spanish_tokenizer, \"english\")\n",
        "    indices = getIndices(tokens, vocab1, vocab2, \"english\")\n",
        "    #print(indices)\n",
        "    tensor = torch.tensor(indices, dtype=torch.long).unsqueeze(-1).to(device)\n",
        "    #print(tensor)\n",
        "    hidden, cell = model.encoder(tensor)\n",
        "    spanish = [vocab2['<sos>']]\n",
        "\n",
        "    \"\"\"encoder_output, hidden = model.encoder(tensor, len(tokens))\n",
        "    spanish_vocab = {i: word for word, i in spanish_vocab.items()}\n",
        "    decoder_input = torch.tensor([vocab2['<sos>']]).to(device)\n",
        "    spanish = [] \"\"\"\n",
        "\n",
        "    for i in range(50):\n",
        "        tensorInput = torch.tensor([spanish[-1]], dtype=torch.long).to(device)\n",
        "        decoder_output, hidden, cell = model.decoder(tensorInput, hidden, cell)\n",
        "        top1 = decoder_output.argmax(-1).item()\n",
        "        if top1 == vocab2['<eos>']:\n",
        "          spanish.append(top1)\n",
        "          break\n",
        "        spanish.append(top1)\n",
        "        decoder_input = torch.tensor([top1]).to(device)\n",
        "    tokens = vocab2.lookup_tokens(spanish)\n",
        "  return tokens\n",
        "\n",
        "\n",
        "#train(RNN_model, dataLoad, criterion, optimizer, device, 10, 0.5)\n",
        "# I am taking my car to the library.\n",
        "# My sister is my best friend and we enjoy playing games together.\n",
        "# Hi there! How was your day?\n",
        "# My dog is running towards me.\n",
        "# I want to go to the beach.\n",
        "#sentence = \" I want to go to the beach.\"\n",
        "#translated_sentence = translation(RNN_model, sentence, vocab1, vocab2, device)\n",
        "#print(translated_sentence)"
      ],
      "metadata": {
        "id": "Pxm8K3KnVwJ7"
      },
      "execution_count": 171,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"I want to go to the beach.\"\n",
        "translated_sentence = translation(RNN_model, sentence, vocab1, vocab2, device)\n",
        "print(translated_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0O_mttcYdW5H",
        "outputId": "d3777e4c-5fd5-4396-933b-531c9ef69773"
      },
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<sos>', 'quiero', 'quiero', 'en', 'la', '.', '.', '.', '<eos>']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"I am taking my car to the library\"\n",
        "translated_sentence = translation(RNN_model, sentence, vocab1, vocab2, device)\n",
        "print(translated_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0b_dWAu8dZvG",
        "outputId": "947682a0-27be-4856-80b2-28920daeb005"
      },
      "execution_count": 175,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<sos>', 'estoy', 'un', 'mi', 'de', 'en', 'mi', '.', '.', '.', '.', '.', '<eos>']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"My sister is my best friend and we enjoy playing games together.\"\n",
        "translated_sentence = translation(RNN_model, sentence, vocab1, vocab2, device)\n",
        "print(translated_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZYOxoP9OdfQ6",
        "outputId": "d606e83e-4dc8-49a1-ea33-b5121a957ecf"
      },
      "execution_count": 176,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<sos>', 'mi', 'madre', 'que', 'no', ',', 'que', 'de', 'un', 'un', 'un', '.', '.', '.', '.', '.', '.', '.', '.', '.', '<eos>']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"Hi there! How was your day?\"\n",
        "translated_sentence = translation(RNN_model, sentence, vocab1, vocab2, device)\n",
        "print(translated_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0PcrxBoPdmq2",
        "outputId": "9261af90-830d-46db-9e8e-a725b48f6182"
      },
      "execution_count": 177,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<sos>', '¿', 'alguna', '¿', 'alguna', 'es', 'alguna', 'tuyos', 'este', 'su', 'ella', 'a', '<eos>']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"My dog is running towards me.\"\n",
        "translated_sentence = translation(RNN_model, sentence, vocab1, vocab2, device)\n",
        "print(translated_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZV6FrnQ2dukJ",
        "outputId": "79b961db-7723-4904-e9f3-8852051a349d"
      },
      "execution_count": 178,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<sos>', 'mi', 'mi', 'es', 'para', 'se', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '<eos>']\n"
          ]
        }
      ]
    }
  ]
}