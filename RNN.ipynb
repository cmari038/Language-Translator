{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPSCtjTXP0IuhxDgPgEt2Zf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cmari038/Language-Translator/blob/main/RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision torchaudio\n",
        "!pip install spacy\n",
        "#!pip install collections"
      ],
      "metadata": {
        "id": "MmcSj7k0iOb2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy download es_core_news_sm"
      ],
      "metadata": {
        "id": "xqCfxMAWw9R1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd76e812-cba6-41fd-bc78-c7e2cf9d1394"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-sm==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.7.1) (3.7.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.4)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.7.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.25.2)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.12.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.6.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.1)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Collecting es-core-news-sm==3.7.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.7.0/es_core_news_sm-3.7.0-py3-none-any.whl (12.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m43.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from es-core-news-sm==3.7.0) (3.7.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.9.4)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (4.66.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.7.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (24.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.4.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.25.2)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.18.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (4.12.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2024.6.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.1.5)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.1.1)\n",
            "Installing collected packages: es-core-news-sm\n",
            "Successfully installed es-core-news-sm-3.7.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('es_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchtext; torchtext.disable_torchtext_deprecation_warning()\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import spacy\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as functional\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torch.utils.data import random_split\n",
        "from torchtext.vocab import vocab\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from collections import Counter, OrderedDict"
      ],
      "metadata": {
        "id": "oiDcQWEB7eTM"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# connect pytorch to cuda\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"Cuda activated\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "dataset = 'https://raw.githubusercontent.com/cmari038/Language-Translator/main/data.csv'\n",
        "data = pd.read_csv(dataset)\n",
        "\n",
        "#spacy.load('en_core_web_sm')\n",
        "#spacy.load('es_core_news_sm')\n",
        "\n",
        "# processing data\n",
        "\n",
        "english_tokenizer = get_tokenizer('spacy', language = 'en_core_web_sm')\n",
        "spanish_tokenizer = get_tokenizer('spacy', language= 'es_core_news_sm')\n",
        "\n",
        "#print(data)\n",
        "\n",
        "#train = data.sample(frac=0.7)\n",
        "#validate = data.drop(train.index).sample(frac=0.1)\n",
        "#test = data.drop(validate.index)\n",
        "\n",
        "train = data.iloc[:int(len(data) * 0.75)]\n",
        "\n",
        "en_lowercase = []\n",
        "for element in train['english']:\n",
        "  en_lowercase.append(element.lower())\n",
        "\n",
        "train['english'] = en_lowercase\n",
        "\n",
        "es_lowercase = []\n",
        "for element in train['spanish']:\n",
        "  es_lowercase.append(element.lower())\n",
        "\n",
        "train['spanish'] = es_lowercase\n",
        "\n",
        "#train.reset_index(drop=True, inplace=True)\n",
        "#validate.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# Build vocab structure to store tokens and their corresponding index\n",
        "counter1 = Counter()\n",
        "counter2 = Counter()\n",
        "\n",
        "for sentence in train['english']:\n",
        "  counter1.update(english_tokenizer(sentence))\n",
        "\n",
        "for sentence in train['spanish']:\n",
        "  counter2.update(spanish_tokenizer(sentence))\n",
        "\n",
        "en_dict = OrderedDict(counter1.most_common())\n",
        "es_dict = OrderedDict(counter2.most_common())\n",
        "\n",
        "vocab1 = vocab(en_dict, specials = ['<unk>', '<pad>', '<sos>', '<eos>'])\n",
        "vocab2 = vocab(es_dict, specials = ['<unk>', '<pad>', '<sos>', '<eos>'])\n",
        "\n",
        "vocab1.set_default_index(vocab1['<unk>'])\n",
        "vocab2.set_default_index(vocab2['<unk>'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tV0KWu9MilTU",
        "outputId": "7c548e9f-dac4-4084-cef8-0e7d956f00ee"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cuda activated\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/spacy/util.py:1740: UserWarning: [W111] Jupyter notebook detected: if using `prefer_gpu()` or `require_gpu()`, include it in the same cell right before `spacy.load()` to ensure that the model is loaded on the correct device. More information: http://spacy.io/usage/v3#jupyter-notebook-gpu\n",
            "  warnings.warn(Warnings.W111)\n",
            "<ipython-input-62-2ee575e0010d>:31: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  train['english'] = en_lowercase\n",
            "<ipython-input-62-2ee575e0010d>:37: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  train['spanish'] = es_lowercase\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GrMEtXyVOpEf",
        "outputId": "f88c347f-41da-49c8-c302-701611cc6db6"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                      english  \\\n",
            "0                                         go.   \n",
            "1                                         go.   \n",
            "2                                         go.   \n",
            "3                                         go.   \n",
            "4                                         hi.   \n",
            "...                                       ...   \n",
            "89218  tom acknowledges that he was defeated.   \n",
            "89219  tom admitted to spilling the red wine.   \n",
            "89220  tom adopted our method of bookkeeping.   \n",
            "89221  tom agreed to help mary with her work.   \n",
            "89222  tom almost always goes to work by car.   \n",
            "\n",
            "                                                 spanish  \n",
            "0                                                    ve.  \n",
            "1                                                  vete.  \n",
            "2                                                  vaya.  \n",
            "3                                                váyase.  \n",
            "4                                                  hola.  \n",
            "...                                                  ...  \n",
            "89218                    tom reconoce que fue derrotado.  \n",
            "89219         tom admitió haber derramado el vino tinto.  \n",
            "89220         tom adoptó nuestro método de contabilidad.  \n",
            "89221  tom estuvo de acuerdo en ayudar a mary con su ...  \n",
            "89222            tom casi siempre va al trabajo en auto.  \n",
            "\n",
            "[89223 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# finding set of tokens for each phrase\n",
        "def getTokens(df, en_tokenizer, es_tokenizer, lang):\n",
        "  if lang == \"english\":\n",
        "    en_tokens = []\n",
        "\n",
        "    for token in en_tokenizer(df):\n",
        "      en_tokens.append(token)\n",
        "\n",
        "    en_tokens = ['<sos>'] + en_tokens + ['<eos>']\n",
        "\n",
        "    return en_tokens\n",
        "\n",
        "  else:\n",
        "    es_tokens = []\n",
        "\n",
        "    for token in es_tokenizer(df):\n",
        "        es_tokens.append(token)\n",
        "\n",
        "    es_tokens = ['sos'] + es_tokens + ['<eos>']\n",
        "\n",
        "    return es_tokens\n",
        "\n",
        "\n",
        "#token_dict = {\"en_tokenizer\": english_tokenizer, \"es_tokenizer\": spanish_tokenizer}\n",
        "#train = train.apply(map(lambda col: col.map(getTokens)))\n",
        "en_tokens = []\n",
        "es_tokens = []\n",
        "\n",
        "for element in train['english']:\n",
        "  tmp1 = getTokens(element, english_tokenizer, spanish_tokenizer, \"english\")\n",
        "  en_tokens.append(tmp1)\n",
        "\n",
        "for element in train[\"spanish\"]:\n",
        "  tmp2 = getTokens(element, english_tokenizer, spanish_tokenizer, \"spanish\")\n",
        "  es_tokens.append(tmp2)\n",
        "\n",
        "train['en_tokens'] = en_tokens\n",
        "train['es_tokens'] = es_tokens\n",
        "\n",
        "#print(train['en_tokens'])"
      ],
      "metadata": {
        "id": "32pkZX7Z7RWE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5882ab69-2a77-46af-c3ce-73a66b5ac3a3"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-66-d2fb3b41a86f>:37: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  train['en_tokens'] = en_tokens\n",
            "<ipython-input-66-d2fb3b41a86f>:38: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  train['es_tokens'] = es_tokens\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train['en_tokens'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y23V_B__irUt",
        "outputId": "3b98b2c3-e571-4dd1-9219-097c94fa15d7"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0                                    [<sos>, go, ., <eos>]\n",
            "1                                    [<sos>, go, ., <eos>]\n",
            "2                                    [<sos>, go, ., <eos>]\n",
            "3                                    [<sos>, go, ., <eos>]\n",
            "4                                    [<sos>, hi, ., <eos>]\n",
            "                               ...                        \n",
            "89218    [<sos>, tom, acknowledges, that, he, was, defe...\n",
            "89219    [<sos>, tom, admitted, to, spilling, the, red,...\n",
            "89220    [<sos>, tom, adopted, our, method, of, bookkee...\n",
            "89221    [<sos>, tom, agreed, to, help, mary, with, her...\n",
            "89222    [<sos>, tom, almost, always, goes, to, work, b...\n",
            "Name: en_tokens, Length: 89223, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def getIndices(df, en_vocab, es_vocab, lang):\n",
        "  if lang == \"english\":\n",
        "    en_indices = []\n",
        "\n",
        "    for word in df:\n",
        "      en_indices.append(en_vocab[word])\n",
        "\n",
        "    return en_indices\n",
        "\n",
        "  else:\n",
        "    es_indices = []\n",
        "\n",
        "    for word in df:\n",
        "        es_indices.append(es_vocab[word])\n",
        "\n",
        "    return es_indices\n",
        "\n",
        "en_indices = []\n",
        "es_indices = []\n",
        "\n",
        "for element in train[\"en_tokens\"]:\n",
        "  tmp1 = getIndices(element, vocab1, vocab2, \"english\")\n",
        "  en_indices.append(tmp1)\n",
        "\n",
        "for element in train[\"es_tokens\"]:\n",
        "  tmp1 = getIndices(element, vocab1, vocab2, \"spanish\")\n",
        "  es_indices.append(tmp1)\n",
        "\n",
        "train['es_indices'] = es_indices\n",
        "train['en_indices']= en_indices\n",
        "\n",
        "\"\"\"\n",
        "print(train['en_indices'])\n",
        "\n",
        "for series in train['en_indices']:\n",
        "  en_tensor.append(torch.tensor(series, dtype=torch.long))\n",
        "\n",
        "for series in train['es_indices']:\n",
        "  es_tensor.append(torch.tensor(series, dtype=torch.long))\n",
        "\n",
        "train['en_indices'] = en_tensor\n",
        "train['es_indices'] = es_tensor\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "-xUWIEhSSD7Z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        },
        "outputId": "fd3f1505-0706-472b-a5b2-934d4a57f271"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-68-895ec81c67b4>:29: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  train['es_indices'] = es_indices\n",
            "<ipython-input-68-895ec81c67b4>:30: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  train['en_indices']= en_indices\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nprint(train['en_indices'])\\n\\nfor series in train['en_indices']:\\n  en_tensor.append(torch.tensor(series, dtype=torch.long))\\n\\nfor series in train['es_indices']:\\n  es_tensor.append(torch.tensor(series, dtype=torch.long))\\n\\ntrain['en_indices'] = en_tensor\\ntrain['es_indices'] = es_tensor\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train['en_indices'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6BHIg16OjM5w",
        "outputId": "80a1b7fb-e5c4-4f50-a3b6-ecbd1134f8bd"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0                                      [2, 48, 4, 3]\n",
            "1                                      [2, 48, 4, 3]\n",
            "2                                      [2, 48, 4, 3]\n",
            "3                                      [2, 48, 4, 3]\n",
            "4                                    [2, 1711, 4, 3]\n",
            "                            ...                     \n",
            "89218          [2, 8, 10160, 19, 15, 21, 3277, 4, 3]\n",
            "89219      [2, 8, 1383, 7, 10161, 9, 404, 471, 4, 3]\n",
            "89220        [2, 8, 1725, 143, 3185, 26, 5806, 4, 3]\n",
            "89221      [2, 8, 993, 7, 87, 35, 45, 55, 103, 4, 3]\n",
            "89222    [2, 8, 271, 147, 433, 7, 103, 91, 97, 4, 3]\n",
            "Name: en_indices, Length: 89223, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# convets dataframe to dataset for tensor acess\n",
        "class TensorSet(Dataset):\n",
        "  def __init__(self, df):\n",
        "    self.df = df\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.df)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    return torch.tensor(self.df['en_indices'].iloc[index], dtype=torch.long), torch.tensor(self.df['es_indices'].iloc[index], dtype=torch.long)\n",
        "\n",
        "def collate_fn(batch):\n",
        "  # used for making sure sequences are similar lengths by adding tokens to pad out the length\n",
        "  en_batch = []\n",
        "  es_batch = []\n",
        "  for en_sample, es_sample in batch:\n",
        "    en_batch.append(en_sample)\n",
        "    es_batch.append(es_sample)\n",
        "\n",
        "  en_batch = pad_sequence(en_batch, padding_value=vocab1['<pad>'])\n",
        "  es_batch = pad_sequence(es_batch, padding_value=vocab2['<pad>'])\n",
        "\n",
        "  return en_batch, es_batch\n",
        "\n",
        "#tensorSet = TensorSet(train, english_tokenizer, spanish_tokenizer, vocab1, vocab2)\n",
        "tensorSet = TensorSet(train)\n",
        "batch_size = 128\n",
        "dataLoad = DataLoader(tensorSet, shuffle=True, collate_fn=collate_fn, batch_size=batch_size)\n",
        "#evalDataLoad = DataLoader(TensorSet(validate), shuffle=True, collate_fn=collate_fn, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "Zh8-rGf-GpvV"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RNN model\n",
        "# made up of encoder, deocder, and seq2seq\n",
        "# encoder converts tensors into context vectors\n",
        "class RNN_Encoder(nn.Module):\n",
        "    def __init__(self, input, embedding_dimension, hidden_dimension, layers, dropout_p=0.5):\n",
        "        super(RNN_Encoder, self).__init__()\n",
        "        self.hidden_dimension = hidden_dimension\n",
        "        self.layers = layers\n",
        "        self.embed = nn.Embedding(input, embedding_dimension)\n",
        "        self.gru = nn.LSTM(embedding_dimension, hidden_dimension, layers, dropout=dropout_p)\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "    def forward(self, english):\n",
        "        embedded = self.dropout(self.embed(english))\n",
        "        output, (hidden, cell) = self.gru(embedded)\n",
        "        return hidden, cell\n",
        "\n",
        "# decoder uses context vectors to output spanish translation\n",
        "class RNN_Decoder(nn.Module):\n",
        "    def __init__(self, output, embedding_dimension, hidden_dimension, layers, dropout_p=0.5):\n",
        "      super(RNN_Decoder, self).__init__()\n",
        "      self.output = output\n",
        "      self.hidden_dimension = hidden_dimension\n",
        "      self.layers = layers\n",
        "      self.embed = nn.Embedding(output, hidden_dimension)\n",
        "      self.gru = nn.LSTM(embedding_dimension, hidden_dimension, layers, dropout=dropout_p)\n",
        "      self.fc_out = nn.Linear(hidden_dimension, output)\n",
        "      self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "    def forward(self, input, hidden, cell):\n",
        "      input = input.unsqueeze(0)\n",
        "      embedded = self.dropout(self.embed(input))\n",
        "      out, (hidden, cell) = self.gru(embedded, (hidden, cell))\n",
        "      prediction = self.fc_out(out.squeeze(0))\n",
        "      #output = self.embed(input)\n",
        "      #output = functional.relu(output)\n",
        "      #output, hidden = self.gru(output, hidden)\n",
        "      return prediction, hidden, cell\n",
        "\n",
        "# sequence to sequence that ties encoder and decoder together\n",
        "class Sequence(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "      super(Sequence, self).__init__()\n",
        "      self.encoder = encoder\n",
        "      self.decoder = decoder\n",
        "      self.device = device\n",
        "\n",
        "    def forward(self, english, spanish, teacher_forcing_ratio):\n",
        "      batch_size = spanish.shape[1]\n",
        "      es_length = spanish.shape[0]\n",
        "      es_vocab_size = self.decoder.output\n",
        "      outputs = torch.zeros(es_length, batch_size,es_vocab_size).to(self.device)\n",
        "      hidden, cell = self.encoder(english)\n",
        "      input = spanish[0,:]\n",
        "\n",
        "      for i in range(1, es_length):\n",
        "        output, hidden, cell = self.decoder(input, hidden, cell)\n",
        "        outputs[i] = output\n",
        "        teacher_force = np.random.random() < teacher_forcing_ratio\n",
        "        top1 = output.argmax(1)\n",
        "        decoder_input = spanish[i] if teacher_force else top1\n",
        "\n",
        "      return outputs\n",
        "\n",
        "\n",
        "encoder = RNN_Encoder(input=len(vocab1), embedding_dimension=256, hidden_dimension=256, layers=2)\n",
        "decoder = RNN_Decoder(output=len(vocab2), embedding_dimension=256, hidden_dimension=256, layers=2)\n",
        "RNN_model = Sequence(encoder, decoder, device).to(device)"
      ],
      "metadata": {
        "id": "o-Bcb9Nspvh4"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss(ignore_index=vocab2['<pad>'])\n",
        "optimizer = torch.optim.Adam(RNN_model.parameters())\n",
        "\n",
        "def weights(model):\n",
        "  for name, parameter in model.named_parameters():\n",
        "    nn.init.uniform_(parameter.data, -0.8, 0.8)\n",
        "\n",
        "def training(RNN_model, dataLoad, criterion, optimizer, device, epochs, teacher_forcing_ratio, clip):\n",
        "  #RNN_model.train()\n",
        "  for epoch in range(epochs):\n",
        "    RNN_model.train()\n",
        "    epoch_loss = 0\n",
        "    for i, (english, spanish) in enumerate(dataLoad): # iterate in batches through dataloader\n",
        "      english = english.to(device)\n",
        "      spanish = spanish.to(device)\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      output = RNN_model(english, spanish, teacher_forcing_ratio)\n",
        "      output = output[1:].view(-1, output.shape[-1])\n",
        "      spanish = spanish[1:].view(-1)\n",
        "\n",
        "      loss = criterion(output, spanish)\n",
        "      loss.backward()\n",
        "      torch.nn.utils.clip_grad_norm_(RNN_model.parameters(),clip)\n",
        "      optimizer.step()\n",
        "      epoch_loss += loss.item()\n",
        "\n",
        "RNN_model.apply(weights)\n",
        "training(RNN_model, dataLoad, criterion, optimizer, device, 5, 0.5, 1.0)"
      ],
      "metadata": {
        "id": "zVJg6uIts0-D"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# translates an input sentece into spanish using trained model\n",
        "def translation(model, input, vocab1, vocab2, device):\n",
        "  input = input.lower()\n",
        "  model.eval()\n",
        "  #tokens = english_tokenizer(input)\n",
        "  #indices = []\n",
        "  \"\"\"for token in tokens:\n",
        "      indices.append(vocab1[token])\n",
        "  input_tensor = torch.tensor([vocab1['<sos>']] + indices + [vocab1['<eos>']], dtype=torch.long) \"\"\"\n",
        "\n",
        "  with torch.no_grad():\n",
        "    tokens = getTokens(input, english_tokenizer, spanish_tokenizer, \"english\")\n",
        "    indices = getIndices(tokens, vocab1, vocab2, \"english\")\n",
        "    #print(indices)\n",
        "    tensor = torch.tensor(indices, dtype=torch.long).unsqueeze(-1).to(device)\n",
        "    #print(tensor)\n",
        "    hidden, cell = model.encoder(tensor)\n",
        "    spanish = [vocab2['<sos>']]\n",
        "\n",
        "    \"\"\"encoder_output, hidden = model.encoder(tensor, len(tokens))\n",
        "    spanish_vocab = {i: word for word, i in spanish_vocab.items()}\n",
        "    decoder_input = torch.tensor([vocab2['<sos>']]).to(device)\n",
        "    spanish = [] \"\"\"\n",
        "\n",
        "    for i in range(50):\n",
        "        tensorInput = torch.tensor([spanish[-1]], dtype=torch.long).to(device)\n",
        "        decoder_output, hidden, cell = model.decoder(tensorInput, hidden, cell)\n",
        "        top1 = decoder_output.argmax(-1).item()\n",
        "        if top1 == vocab2['<eos>']:\n",
        "          spanish.append(top1)\n",
        "          break\n",
        "        spanish.append(top1)\n",
        "        decoder_input = torch.tensor([top1]).to(device)\n",
        "    tokens = vocab2.lookup_tokens(spanish)\n",
        "  return tokens\n",
        "\n",
        "\n",
        "#train(RNN_model, dataLoad, criterion, optimizer, device, 10, 0.5)\n",
        "sentence = \"The dog is running towards me\"\n",
        "translated_sentence = translation(RNN_model, sentence, vocab1, vocab2, device)\n",
        "print(translated_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pxm8K3KnVwJ7",
        "outputId": "28f20c70-0218-44c6-dce4-aae3d7691ef7"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<sos>', 'él', 'tenemos', 'éxitos', 'ellos', 'me', '.', ',', 'que', '.', '<eos>']\n"
          ]
        }
      ]
    }
  ]
}